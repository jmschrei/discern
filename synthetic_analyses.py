# synthetic_analyses.py
# Contact: Jacob Schreiber
#          jmschr@cs.washington.edu

'''
These tests will show the difference between J-DISCERN, M-DISCERN, and 
ANOVA on data generated by synthetic networks. 
'''

import matplotlib
matplotlib.use('svg')

import numpy
import random
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from yabn import *
from discern import *
from LNS import *
from scipy.stats import f_oneway

random.seed(0)
numpy.random.seed(0)


def barchart( scores, method_names, node_names, title ):
	'''
	Take in the scores from two different feature selectors and plot them.
	'''

	sns.set( style='white', context='talk' )
	plt.figure( figsize=(12, 6) )
	n = len( scores )
	
	items = zip( xrange(n), scores, method_names, sns.color_palette('husl', 3) )
	for i, score, name, color in items:
		score /= score.sum()

		x = np.arange( 0.5, score.shape[0]+0.5 )
		plt.bar( x+i*(0.8/n), score, width=0.8/n, alpha=0.5, edgecolor='w', label=name, facecolor=color )

	plt.legend()
	plt.title( title )
	plt.xticks( x+(1.0/n), node_names )
	plt.savefig( title + '.svg' )

def score_network_pair( networka, networkb, node_names, i=100, j=100 ):
	'''
	This will take in a network and produce DISCERN and ANOVA scores for
	each node in the network. The user may set the number of samples
	generated for each network through adjusting i and j. Pass in the
	order of the node names to get the scores in the proper order.
	'''

	node_names_a = [ node.name for node in networka.nodes ]
	node_names_b = [ node.name for node in networkb.nodes ]

	print "sampling"
	# Get the data from sampling the two networks
	a_data = numpy.array([ networka.sample() for i in xrange( 100 ) ])
	b_data = numpy.array([ networkb.sample() for i in xrange( 100 ) ])

	# Convert this data into a dataframe for DISCERN
	a_data = pd.DataFrame( a_data, columns=node_names_a )
	b_data = pd.DataFrame( b_data, columns=node_names_b )

	# Initialize DISCERN and use it on the data
	discern = DISCERN( l=0.3 )
	discern.fit_score( a_data, b_data, node_names_a, n_cores=6 )

	# Get the LNS scores
	lns = LNS()
	lns.fit_score( a_data, b_data, node_names_a )

	# Unpack the two score vectors into a numpy array
	discern_scores = numpy.array(discern._scores.ix[ node_names ]['T4'])
	anova_scores = numpy.array([ f_oneway( a_data[name], b_data[name] )[0] for name in node_names ])
	lns_scores = numpy.array( lns._scores.ix[ node_names ]['r'] )

	return discern_scores, anova_scores, lns_scores


def seven_star_tests():
	'''
	These tests work on a star network, where one node influences a second node,
	which then influences three nodes, and there are two independent nods, which
	switch identities in the graph. Basically, an influencer no longer influences
	and an independent node takes its place.
	'''

	# Define the two networks we will use
	networka = Network( "A" )
	networkb = Network( "B" )

	# Define all seven nodes, which are the same between the two networks
	n1 = Node( NormalDistribution( 12, 0.7 ), name="n1" )
	n2 = Node( NormalDistribution( 5, 0.3 ), name="n2" )
	n3 = Node( NormalDistribution( 17, 0.9 ), name="n3" )
	n4 = Node( NormalDistribution( 22, 1.2 ), name="n4" )
	n5 = Node( NormalDistribution( 12, 0.3 ), name="n5" )
	n6 = Node( NormalDistribution( 27, 3.2 ), name="n6" )
	n7 = Node( NormalDistribution( 88, 1.2 ), name="n7" )

	# We'll use a single edge of unit variance for this simple test
	e = Edge( NormalDistribution( 0, 0.2 ), weight=1 )

	# Add all the nodes to the networks
	networka.add_nodes( [n1, n2, n3, n4, n5, n6, n7] )
	networkb.add_nodes( [n1, n2, n3, n4, n5, n6, n7] )

	# Add all the edges to network A
	networka.add_edge( n1, n3, e )
	networka.add_edge( n3, n5, e )
	networka.add_edge( n3, n6, e )
	networka.add_edge( n3, n7, e )

	# Add all the edges to network B
	networkb.add_edge( n4, n3, e )
	networkb.add_edge( n3, n5, e )
	networkb.add_edge( n3, n6, e )
	networkb.add_edge( n3, n7, e )

	# Finalize the internals of the models
	networka.bake()
	networkb.bake()

	# Define the ordered names
	node_names = [ "n1", "n2", "n3", "n4", "n5", "n6", "n7" ]

	# Score the network
	discern, anova, lns = score_network_pair( networka, networkb, node_names, 
		['DISCERN', 'ANOVA', 'LNS'] )

	# Plot the scores
	barchart( [discern, anova, lns], ['DISCERN', 'ANOVA', 'LNS'], node_names, "n4-n3+ n1-n3-" )


	# Time for a second test, involving a network where only an edge between
	# n4 and n1 is added and nothing is removed.
	networkb = Network( 'b' )

	# Add the nodes in
	networkb.add_nodes( [n1, n2, n3, n4, n5, n6, n7] )

	# Add the edges in
	networkb.add_edge( n1, n3, e )
	networkb.add_edge( n3, n5, e )
	networkb.add_edge( n3, n6, e )
	networkb.add_edge( n3, n7, e )
	networkb.add_edge( n4, n1, e )

	# Finalize the model
	networkb.bake()

	# Score the nodes
	discern, anova, lns = score_network_pair( networka, networkb, node_names, 
		['DISCERN', 'ANOVA', 'LNS'] )

	# Plot the scores
	barchart( [discern, anova, lns], ['DISCERN', 'ANOVA', 'LNS'], node_names, "n4-n1+" )

def independent_no_perturbation_test( name="independent" ):
	''' 
	This will test a network which has no edges, and no perturbation, to see
	that the prediction power is not random.
	'''

	network = Network( 'independent' )

	# Create 12 distributions of random size 
	n1 = Node( NormalDistribution( 5, 9 ), name="n1" )
	n2 = Node( NormalDistribution( 12, 3 ), name="n2" )
	n3 = Node( NormalDistribution( 10, 2 ), name="n3" )
	n4 = Node( NormalDistribution( 12, 3 ), name="n4" )
	n5 = Node( NormalDistribution( 18, 1 ), name="n5" )
	n6 = Node( NormalDistribution( 100, 1 ), name="n6" )
	n7 = Node( NormalDistribution( 2, 0.0001 ), name="n7" )
	n8 = Node( NormalDistribution( 56, 12 ), name="n8" )
	n9 = Node( NormalDistribution( 144, 2 ), name="n9" )
	n10 = Node( NormalDistribution( 87, 22 ), name="n10" )
	n11 = Node( NormalDistribution( 5, 0.5 ), name="n11" )
	n12 = Node( NormalDistribution( 46, 0.3 ), name="n12" )

	# Add the nodes and finalize the structure of the data
	network.add_nodes( [n1,n2,n3,n4,n5,n6,n7,n8,n9,n10,n11,n12] )
	network.bake()

	node_names = [ 'n{}'.format( i ) for i in xrange( 1, 13 ) ]

	# Get the scores
	discern, anova, lns = score_network_pair( network, network, node_names )

	# Plot it
	barchart( [discern, anova, lns], ['DISCERN', 'ANOVA', 'LNS'], node_names, name )

def three_component_test( name="three_component"):
	'''
	This will test a network which is large and has many perturbations
	to see how well DISCERN can do.
	'''

	networka = Network( 'a' )
	networkb = Network( 'b' )

	# Create some nodes
	'''
	n1 = Node( NormalDistribution( 4, 0.5 ), name="n1" )
	n2 = Node( NormalDistribution( 8, 1.2 ), name="n2" )
	n3 = Node( NormalDistribution( 12, 7.2 ), name="n3" )
	n4 = Node( NormalDistribution( 18, 3.0 ), name="n4" )
	n5 = Node( NormalDistribution( 62, 3.4 ), name="n5" )
	n6 = Node( NormalDistribution( 34, 1.2 ), name="n6" )
	n7 = Node( NormalDistribution( 101, 5 ), name="n7" )
	n8 = Node( NormalDistribution( 88, 12 ), name="n8" )
	n9 = Node( NormalDistribution( 72, 18 ), name="n9" )
	n10 = Node( NormalDistribution( 49, 10 ), name="n10" )
	n11 = Node( NormalDistribution( 1, 0.07 ), name="n11" )
	n12 = Node( NormalDistribution( 34, 0.03 ), name="n12" )
	n13 = Node( NormalDistribution( 4, 0.5 ), name="n13" )
	'''
	emission = NormalDistribution( 10, 1 )
	n1 = Node( emission, name="n1" )
	n2 = Node( emission, name="n2" )
	n3 = Node( emission, name="n3" )
	n4 = Node( emission, name="n4" )
	n5 = Node( emission, name="n5" )
	n6 = Node( emission, name="n6" )
	n7 = Node( emission, name="n7" )
	n8 = Node( emission, name="n8" )
	n9 = Node( emission, name="n9" )
	n10 = Node( emission, name="n10" )
	n11 = Node( emission, name="n11" )
	n12 = Node( emission, name="n12" )
	n13 = Node( emission, name="n13" )


	# Unpack nodes
	node_names = [ 'n{}'.format( i ) for i in xrange( 1, 14 ) ]

	# Add the nodes to the module
	networka.add_nodes( [n1,n2,n3,n4,n5,n6,n7,n8,n9,n10,n11,n12,n13] )
	networkb.add_nodes( [n1,n2,n3,n4,n5,n6,n7,n8,n9,n10,n11,n12,n13] )

	# Define a uniform edge for simplicity
	e = Edge( NormalDistribution( 0, 1 ), weight=1.0 )

	# Add edges to the models
	networka.add_edge( n1, n2, e )
	networka.add_edge( n2, n3, e )
	networka.add_edge( n4, n2, e )
	networka.add_edge( n5, n6, e )
	networka.add_edge( n6, n7, e )
	networka.add_edge( n7, n9, e )
	networka.add_edge( n7, n10, e )
	networka.add_edge( n12, n11, e )
	networka.add_edge( n13, n12, e )

	networkb.add_edge( n1, n2, e )
	networkb.add_edge( n4, n2, e )
	networkb.add_edge( n5, n6, e )
	networkb.add_edge( n6, n7, e )
	networkb.add_edge( n7, n9, e )
	networkb.add_edge( n7, n10, e )
	networkb.add_edge( n12, n11, e )
	networkb.add_edge( n13, n12, e )
	networkb.add_edge( n4, n11, e )
	networkb.add_edge( n5, n8, e )
	networkb.add_edge( n8, n7, e )

	# Finalize the models
	networka.bake()
	networkb.bake()

	discern, anova, lns = score_network_pair( networka, networkb, node_names,
		['DISCERN', 'ANOVA', 'LNS'] )

	barchart( [discern, anova, lns], ['DISCERN', 'ANOVA', 'LNS'], node_names, name )

def large_sparse_network( n=5000, m=50, low=1, high=5, name="large_sparse" ):
	'''
	Create a synthetic large, sparse network of randomly generated emissions
	and edge weights. 
	'''

	# Randomly generate normal distributions for the node emissions
	# Means based on a gamma distribution, stds based on a lognormal
	# so that they are both bounded by 1
	means = numpy.random.gamma( 50, 3.0, n )
	stds = numpy.random.lognormal( 0.5, 0.1, n )

	# Randomly choose M genes to perturb, and then for each perturbed gene
	# randomly choose the number of edges to perturb
	perturbed = numpy.random.choice( np.arange( n ), size=m, replace=False )
	n_perturbed_edges = numpy.random.randint( low, high, m )

	# Randomly generate the graph structure from beta distributions. All
	# weights are rounded to 1, instead of being variable.
	null_edges = numpy.tril( numpy.around( numpy.random.beta( 1, 3, (n,n) ) ) )
	numpy.fill_diagonal( null_edges, 0 )
	print null_edges.sum()
	alternate_edges = null_edges.copy()

	perturb_count = { i:0 for i in xrange(n) }

	# For each perturbed edge, randomly select between `low` and `high` number
	# of edges to perturb, and perturb them--in this case just a binary flip.
	for i, k in it.izip( perturbed, n_perturbed_edges ):
		perturbed_id = numpy.random.choice( numpy.arange( i+1,n ), size=min(k, n-i-1), replace=False )
		alternate_edges[perturbed_id, i] = numpy.abs( alternate_edges[ perturbed_id, i] - 1 )

		perturb_count[i] += perturbed_id.shape[0]
		for index in perturbed_id:
			perturb_count[index] += 1 
 

	if numpy.triu( alternate_edges ).sum() > 0:
		raise SyntaxError( "Matrix is not a DAG.")

	# Initiate the network objects
	null = Network( "Null" )
	alternate = Network( "Alternate" ) 
	
	# Create all the nodes 
	nodes = [ Node( NormalDistribution( mu, sigma ), name="n{}".format( i ) ) for i, mu, sigma in it.izip( xrange(n), means, stds ) ]
	node_names = [ node.name for node in nodes ]

	# Add them to the model
	null.add_nodes( nodes )
	alternate.add_nodes( nodes )

	# Create all the edges, one at a time
	for i in xrange( n ):
		for j in xrange( n ):
			p = null_edges[i, j]
			if p > 0:
				edge = Edge( NormalDistribution( 0, 1 ), weight=p )
				null.add_edge( nodes[i], nodes[j], edge )

			p = alternate_edges[i, j]
			if p > 0:
				edge = Edge( NormalDistribution( 0, 1 ), weight=p )
				alternate.add_edge( nodes[i], nodes[j], edge )

	# Finalize the internal structure of the network
	null.bake()
	alternate.bake()

	# Score the network pair according to the metrics
	discern, anova, lns = score_network_pair( null, alternate, node_names )

	# Make a plot of the scores acorss the nodes
	#barchart( [discern, anova, lns], ['DISCERN', 'ANOVA', 'LNS'], node_names, name )

	scores = pd.DataFrame({ 'DISCERN': discern, 'ANOVA': anova, 
		'LNS': lns, 'TRUE': perturb_count.values() })
	
	discern_DCG = DCG( scores.sort( 'DISCERN', ascending=False )['TRUE'].values )
	anova_DCG = DCG( scores.sort( 'ANOVA', ascending=False )['TRUE'].values )
	lns_DCG = DCG( scores.sort( 'LNS', ascending=False )['TRUE'].values )


	print discern_DCG, anova_DCG, lns_DCG

def DCG( relevance ):
	'''
	Calculates the Discounted Cumulative Gain of stuff.
	'''
	
	n = len( relevance )
	return sum( (2.**relevance[i]-1.) / np.log2(i+2) for i in xrange( n ) )

large_sparse_network( 100, 10 )
#independent_no_perturbation_test() 
#seven_star_tests()
#three_component_test()
